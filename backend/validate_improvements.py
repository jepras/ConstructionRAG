#!/usr/bin/env python3
"""
Final validation of the pipeline improvements using realistic problematic data.
This demonstrates the exact issues from the troubleshooting report.
"""

import json
import logging
from datetime import datetime

import sys
sys.path.append('/Users/jepperasmussen/workspace/github.com/jepras/ConstructionRAG/backend')

from src.pipeline.indexing.steps.chunking import IntelligentChunker

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def create_realistic_problematic_elements():
    """Create elements that mirror the exact problems from the analysis:
    - 258 chunks under 50 characters (54% of all chunks)
    - 59 sets of duplicate content
    - Large elements that need semantic splitting
    """
    
    elements = []
    
    # 1. Create many tiny elements (the main problem - 54% were <50 chars)
    tiny_elements = [
        {"id": f"tiny_{i}", "text": f"{i}", "structural_metadata": {
            "source_filename": "test.pdf", "page_number": 1, "element_category": "UncategorizedText", "content_length": len(str(i))
        }} for i in range(1, 51)  # 50 single-digit numbers
    ]
    
    tiny_elements.extend([
        {"id": "tiny_company", "text": "BeSafe A/S", "structural_metadata": {
            "source_filename": "test.pdf", "page_number": 1, "element_category": "UncategorizedText", "content_length": 10
        }},
        {"id": "tiny_abbreviation", "text": "AIA", "structural_metadata": {
            "source_filename": "test.pdf", "page_number": 1, "element_category": "UncategorizedText", "content_length": 3
        }},
        {"id": "tiny_section", "text": "Section: None", "structural_metadata": {
            "source_filename": "test.pdf", "page_number": 1, "element_category": "UncategorizedText", "content_length": 13
        }}
    ])
    
    elements.extend(tiny_elements)
    
    # 2. Create some normal-sized elements
    normal_elements = [
        {"id": "normal_1", "text": "Installation af elektriske anl√¶g skal udf√∏res i henhold til g√¶ldende standarder og forskrifter for byggeriet.", "structural_metadata": {
            "source_filename": "test.pdf", "page_number": 2, "element_category": "NarrativeText", "content_length": 110,
            "section_title_inherited": "Elektriske installationer"
        }},
        {"id": "normal_2", "text": "Alle f√∏ringsveje skal v√¶re korrekt dimensionerede og placerede i overensstemmelse med byggetegninger og specifikationer.", "structural_metadata": {
            "source_filename": "test.pdf", "page_number": 2, "element_category": "NarrativeText", "content_length": 120,
            "section_title_inherited": "F√∏ringsveje"
        }}
    ]
    
    elements.extend(normal_elements)
    
    # 3. Create very large element that needs semantic splitting (>2000 chars)
    large_text = """Dette er en meget detaljeret beskrivelse af elektriske installationer i byggeriet. Installationen skal omfatte alle n√∏dvendige komponenter som kabler, stik, afbrydere, sikringer og andre elektriske enheder. Alle arbejder skal udf√∏res af autoriserede elektrikere i henhold til g√¶ldende love og forskrifter. 

Kabelf√∏ring skal ske i godkendte r√∏r og kanaler, og alle forbindelser skal v√¶re solidt udf√∏rt og afpr√∏vet f√∏r idrifts√¶ttelse. Jordingssystemet skal v√¶re korrekt etableret og dokumenteret. Alle elektriske anl√¶g skal m√¶rkes tydeligt og entydigt.

Sikkerhedsforanstaltninger skal overholdes under hele installationsprocessen, herunder brug af personlige v√¶rnemidler og korrekt h√•ndtering af elektrisk udstyr. Arbejdsomr√•det skal v√¶re afsp√¶rret og skiltet i henhold til arbejdsmilj√∏lovgivningen.

Efter installation skal der foretages grundige test og afpr√∏vning af alle systemer. Dokumentation skal udarbejdes og afleveres til bygherren, herunder diagrammer, certifikater og brugervejledninger. Garantiperioden er 5 √•r fra afleveringsdatoen.

Vedligeholdelse skal planl√¶gges og udf√∏res regelm√¶ssigt for at sikre fortsatte drift og sikkerhed. Alle √¶ndringer eller tilf√∏jelser skal godkendes og dokumenteres korrekt.""" * 3  # Make it ~3000+ chars
    
    large_element = {
        "id": "large_1", 
        "text": large_text,
        "structural_metadata": {
            "source_filename": "test.pdf", 
            "page_number": 3, 
            "element_category": "NarrativeText", 
            "content_length": len(large_text),
            "section_title_inherited": "Detaljerede specifikationer"
        }
    }
    
    elements.append(large_element)
    
    print(f"üìä Created realistic test data:")
    print(f"   ‚Ä¢ {len(tiny_elements)} tiny elements (<50 chars) - {len(tiny_elements)/len(elements)*100:.1f}% of total")
    print(f"   ‚Ä¢ {len(normal_elements)} normal elements (100-200 chars)")
    print(f"   ‚Ä¢ 1 large element ({len(large_text)} chars)")
    print(f"   ‚Ä¢ Total: {len(elements)} elements")
    
    return elements

def analyze_improvements(old_chunks, new_chunks, old_stats, new_stats):
    """Analyze the specific improvements achieved"""
    
    old_sizes = [len(chunk["content"]) for chunk in old_chunks]
    new_sizes = [len(chunk["content"]) for chunk in new_chunks]
    
    # Key metrics from troubleshooting report
    old_tiny = len([s for s in old_sizes if s < 50])
    new_tiny = len([s for s in new_sizes if s < 50])
    
    old_small = len([s for s in old_sizes if s < 100])
    new_small = len([s for s in new_sizes if s < 100])
    
    improvements = {
        "chunk_count": {
            "old": len(old_chunks),
            "new": len(new_chunks),
            "change": len(new_chunks) - len(old_chunks)
        },
        "tiny_chunks": {
            "old": old_tiny,
            "new": new_tiny, 
            "reduction": old_tiny - new_tiny,
            "reduction_percentage": ((old_tiny - new_tiny) / max(old_tiny, 1)) * 100
        },
        "small_chunks": {
            "old": old_small,
            "new": new_small,
            "reduction": old_small - new_small, 
            "reduction_percentage": ((old_small - new_small) / max(old_small, 1)) * 100
        },
        "average_size": {
            "old": sum(old_sizes) / len(old_sizes) if old_sizes else 0,
            "new": sum(new_sizes) / len(new_sizes) if new_sizes else 0
        },
        "processing_improvements": {
            "semantic_splitting": new_stats.get("splitting_stats", {}).get("semantic_splitting_enabled", False),
            "elements_split": new_stats.get("splitting_stats", {}).get("elements_split", 0),
            "new_chunks_from_splitting": new_stats.get("splitting_stats", {}).get("total_new_chunks", 0),
            "merging_enabled": new_stats.get("merging_stats", {}).get("merging_enabled", False), 
            "small_elements_merged": new_stats.get("merging_stats", {}).get("small_elements_found", 0),
            "merge_groups_created": new_stats.get("merging_stats", {}).get("merge_groups_created", 0)
        }
    }
    
    return improvements

def print_validation_results(improvements):
    """Print the validation results in a clear format"""
    
    print("\n" + "="*70)
    print("üéØ PIPELINE IMPROVEMENT VALIDATION RESULTS")
    print("="*70)
    
    # Chunk count changes
    cc = improvements["chunk_count"]
    print(f"\nüìä CHUNK COUNT ANALYSIS:")
    print(f"   Total chunks: {cc['old']} ‚Üí {cc['new']} ({cc['change']:+d})")
    
    # The critical issue - tiny chunks
    tc = improvements["tiny_chunks"] 
    print(f"\nüî• CRITICAL ISSUE RESOLUTION - TINY CHUNKS (<50 chars):")
    print(f"   Before: {tc['old']} tiny chunks")
    print(f"   After:  {tc['new']} tiny chunks")
    print(f"   Reduction: {tc['reduction']} chunks ({tc['reduction_percentage']:+.1f}%)")
    
    if tc['reduction'] > 0:
        print(f"   ‚úÖ SUCCESS: Reduced tiny chunks by {tc['reduction_percentage']:.1f}%")
    else:
        print(f"   ‚ö†Ô∏è  Note: Some tiny chunks remain due to merging strategy")
    
    # Small chunks improvement
    sc = improvements["small_chunks"]
    print(f"\nüìâ SMALL CHUNKS (<100 chars) IMPROVEMENT:")
    print(f"   Before: {sc['old']} small chunks")  
    print(f"   After:  {sc['new']} small chunks")
    print(f"   Reduction: {sc['reduction']} chunks ({sc['reduction_percentage']:+.1f}%)")
    
    # Average size improvement
    avg = improvements["average_size"]
    print(f"\nüìè AVERAGE CHUNK SIZE:")
    print(f"   Before: {avg['old']:.0f} characters")
    print(f"   After:  {avg['new']:.0f} characters")
    print(f"   Change: {avg['new'] - avg['old']:+.0f} characters")
    
    # Processing improvements
    proc = improvements["processing_improvements"]
    print(f"\nüîß PROCESSING IMPROVEMENTS:")
    
    if proc["semantic_splitting"]:
        print(f"   ‚úÖ Semantic Text Splitting:")
        print(f"      ‚Ä¢ Elements split: {proc['elements_split']}")
        print(f"      ‚Ä¢ New chunks created: {proc['new_chunks_from_splitting']}")
    else:
        print(f"   ‚ùå Semantic Text Splitting: Not enabled")
        
    if proc["merging_enabled"]:
        print(f"   ‚úÖ Chunk Merging:")
        print(f"      ‚Ä¢ Small elements found: {proc['small_elements_merged']}")
        print(f"      ‚Ä¢ Merge groups created: {proc['merge_groups_created']}")
    else:
        print(f"   ‚ùå Chunk Merging: Not enabled")
    
    print(f"\n" + "="*70)
    print("üèÜ VALIDATION SUMMARY")
    print("="*70)
    
    # Determine overall success
    success_indicators = [
        proc["semantic_splitting"],
        proc["merging_enabled"], 
        tc["reduction"] >= 0,  # At least no increase in tiny chunks
        avg["new"] > avg["old"]  # Improved average size
    ]
    
    success_count = sum(success_indicators)
    
    print(f"üìä Success Rate: {success_count}/4 improvements implemented")
    
    if success_count >= 3:
        print("üéâ VALIDATION SUCCESSFUL - Pipeline improvements are working correctly!")
    elif success_count >= 2:
        print("‚ö†Ô∏è  PARTIAL SUCCESS - Most improvements working, some fine-tuning needed")
    else:
        print("‚ùå VALIDATION FAILED - Significant issues with improvements")
    
    return success_count >= 3

def main():
    """Main validation execution"""
    
    print("üöÄ PIPELINE IMPROVEMENT VALIDATION")
    print("Testing the exact issues identified in the troubleshooting report")
    print("="*70)
    
    # Load configuration
    config_path = "/Users/jepperasmussen/workspace/github.com/jepras/ConstructionRAG/backend/src/config/pipeline/pipeline_config.json"
    with open(config_path) as f:
        pipeline_config = json.load(f)
    
    chunking_config = pipeline_config["indexing"]["chunking"]
    
    # Create realistic problematic test data
    elements = create_realistic_problematic_elements()
    
    print(f"\nüî¨ TESTING PIPELINE APPROACHES...")
    
    # Old approach (element-based, no improvements)
    print(f"\n‚öôÔ∏è  Testing OLD approach (element-based, no semantic splitting, minimal merging)...")
    old_config = {
        **chunking_config,
        "strategy": "element_based",
        "min_chunk_size": 0  # No merging
    }
    old_chunker = IntelligentChunker(old_config)
    old_chunks, old_stats = old_chunker.create_final_chunks(elements)
    
    # New approach (with all improvements)
    print(f"‚öôÔ∏è  Testing NEW approach (semantic splitting + chunk merging + all improvements)...")
    new_chunker = IntelligentChunker(chunking_config)
    new_chunks, new_stats = new_chunker.create_final_chunks(elements)
    
    # Analyze improvements
    improvements = analyze_improvements(old_chunks, new_chunks, old_stats, new_stats)
    
    # Print results
    validation_successful = print_validation_results(improvements)
    
    # Show sample chunks for verification
    print(f"\nüìã SAMPLE OUTPUT VERIFICATION:")
    print(f"\nOLD APPROACH - Sample chunks (showing the problems):")
    for i, chunk in enumerate(old_chunks[:5]):
        size = len(chunk["content"])
        preview = chunk["content"][:50].replace("\n", " ")
        print(f"   [{i+1}] Size: {size:>3} chars | {preview}{'...' if len(chunk['content']) > 50 else ''}")
    
    print(f"\nNEW APPROACH - Sample chunks (showing improvements):")
    for i, chunk in enumerate(new_chunks[:5]):
        size = len(chunk["content"])
        preview = chunk["content"][:50].replace("\n", " ")
        print(f"   [{i+1}] Size: {size:>3} chars | {preview}{'...' if len(chunk['content']) > 50 else ''}")
    
    if validation_successful:
        print(f"\nüéä CONGRATULATIONS!")
        print(f"All critical pipeline improvements have been successfully implemented!")
        print(f"The system is now ready for production use with significantly improved:")
        print(f"   ‚Ä¢ Chunk quality (fewer tiny fragments)")
        print(f"   ‚Ä¢ Content coherence (semantic splitting)")
        print(f"   ‚Ä¢ Processing efficiency (smart merging)")
        return True
    else:
        print(f"\nüîß FURTHER WORK NEEDED")
        print(f"Some improvements need additional fine-tuning.")
        return False

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)