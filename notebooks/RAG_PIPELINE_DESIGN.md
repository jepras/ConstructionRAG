# RAG Pipeline Design Document
## Remaining Implementation Steps & Design Decisions

---

## ğŸ“‹ **Current Pipeline Status**

### âœ… **Completed Steps (1-5)**
- **01_partition**: Document partitioning with vision processing
- **02_meta_data**: Rich metadata extraction and enhancement  
- **03_enrich_data**: Data enrichment from metadata
- **04_chunk**: Intelligent chunking with adaptive strategies
- **05_embed**: Vector embeddings with Voyage AI

### ğŸ”„ **Remaining Steps (6-12)**
- **06_store**: Vector storage with Chroma + metadata indexing + validation
- **07_query**: Query processing and expansion (no intent detection)
- **08_retrieve**: Hybrid search (semantic + keyword) with metadata filtering
- **09_rerank**: Re-ranking retrieved results for relevance optimization
- **10_context**: Context assembly and prompt engineering
- **11_generate**: LLM response generation with OpenAI
- **12_evaluate**: Evaluation framework and metrics

---

## ğŸ—ï¸ **Notebook Structure Guidelines**

### **Standard Notebook Template Structure**

Based on analysis of existing notebooks (`chunking_stripped.py`, `embed_voyage.py`, `meta_data_unified.py`):

```python
# ==============================================================================
# [NOTEBOOK TITLE] - [BRIEF DESCRIPTION]
# [Purpose and context description]
# ==============================================================================

import os
import sys
import pickle
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional

# --- External Libraries ---
# (Import third-party libraries here)

# --- Environment Variables ---
# (Load API keys, environment configs)

# ==============================================================================
# 1. CONFIGURATION
# ==============================================================================

# --- Input Data Configuration ---
PREVIOUS_STEP_RUN = "XX_run_YYYYMMDD_HHMMSS"  # Which previous run to load

# --- Model/Service Configuration ---
# (API keys, model names, parameters)

# --- Path Configuration ---
INPUT_BASE_DIR = "../../data/internal/XX_previous_step"
OUTPUT_BASE_DIR = "../../data/internal/XX_current_step"

# --- Create timestamped output directory ---
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
CURRENT_RUN_DIR = Path(OUTPUT_BASE_DIR) / f"XX_run_{timestamp}"
CURRENT_RUN_DIR.mkdir(parents=True, exist_ok=True)

# --- Processing Configuration ---
# (Load from config file or define parameters)

# ==============================================================================
# 2. DATA MODELS
# ==============================================================================

# (Pydantic models for type safety)

# ==============================================================================
# 3. CORE FUNCTIONS
# ==============================================================================

# (Main processing logic)

# ==============================================================================
# 4. MAIN EXECUTION
# ==============================================================================
```

### **Directory Structure Pattern**
```
notebooks/XX_step_name/
â”œâ”€â”€ main_processing_script.py        # Core implementation
â”œâ”€â”€ config/
â”‚   â””â”€â”€ step_config.json            # Configuration parameters
â”œâ”€â”€ data/                            # Local test data (if needed)
â”œâ”€â”€ README.md                        # Step-specific documentation
â””â”€â”€ [ANALYSIS].md                    # Analysis and findings (optional)
```

### **Data Flow Pattern**
```
data/internal/
â”œâ”€â”€ 01_partition_data/
â”œâ”€â”€ 02_meta_data/
â”œâ”€â”€ 03_enrich_data/
â”œâ”€â”€ 04_chunking/
â”‚   â”œâ”€â”€ 04_run_20250722_134606/      # Timestamped runs
â”‚   â””â”€â”€ 04_run_20250722_134421/
â”œâ”€â”€ 05_embedding/
â”‚   â”œâ”€â”€ 05_voyage_run_20250723_074238/
â”‚   â””â”€â”€ 05_run_20250723_075021/
â””â”€â”€ [06-12]_[step_name]/
    â””â”€â”€ [XX]_run_[timestamp]/
```

---

## ğŸ¯ **Design Decisions for Remaining Steps**

### **06_store - âœ… IMPLEMENTED**

**Comprehensive ChromaDB storage with integrated validation and Danish semantic search testing.**

#### **Implementation**
- **Notebook**: `store_and_validate.py` 
- **Technology**: ChromaDB with persistent storage
- **Input**: Step 05 embedded chunks (auto-detects latest or specify run)
- **Output**: Persistent vector database + validation reports

#### **Key Features**
- **Automatic metadata flattening** for ChromaDB compatibility
- **Batch processing** with configurable batch sizes
- **Integrated validation** across storage, search, and retrieval quality
- **Danish semantic search testing** with construction-specific queries

#### **Configuration**
```python
SPECIFIC_EMBEDDING_RUN = ""  # Auto-detect latest or specify: "05_voyage_run_20250723_074238"
CHROMA_PERSIST_DIRECTORY = "../../chroma_db"
COLLECTION_NAME = "construction_documents"
EMBEDDING_DIMENSION = 1024  # Voyage multilingual-2
```

#### **Validation Tests**
1. **Storage Integrity**: Document count, metadata indexing, embedding dimensions
2. **Search Performance**: 16 bilingual queries (Danish/English) with response time < 500ms
3. **Metadata Filtering**: Source filename and element category filtering
4. **Retrieval Quality**: Danish construction queries ("regnvand", "omkostninger", "projekt information")

#### **Validation Results** 
âœ… **Production Ready**: Average similarity -0.289, excellent Danish semantic search  
âœ… **Performance**: Sub-millisecond response times  
âœ… **Quality**: Clear ranking differentiation (0.2-0.6 similarity range)

#### **Output Files**
- `storage_validation_report.json` - Overall validation summary
- `retrieval_quality_report.json` - Detailed Danish semantic search analysis

### **07_query - Language-Agnostic Query Processing**

#### **LLM-Based Semantic Query Expansion**
```python
def expand_query_semantically(original_query: str) -> List[str]:
    """Generate semantic variations using GPT without hardcoded terms"""
    
    prompt = f"""
    Given this construction/tender query: "{original_query}"
    
    Generate 4 semantically similar queries that could find the same information.
    Consider:
    - Alternative technical terminology
    - Different phrasing styles (formal/informal)  
    - Related concepts that might contain the answer
    - Broader and narrower interpretations
    
    Return only the alternative queries, one per line.
    """
    
    response = openai.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3
    )
    
    return [q.strip() for q in response.choices[0].message.content.strip().split('\n') if q.strip()]
```

#### **Metadata-Driven Query Routing**
```python
def route_query_by_content_type(query: str) -> Dict[str, Any]:
    """Route queries based on semantic content, not language"""
    
    type_prompt = f"""
    Classify this query into one category:
    - quantities (amounts, numbers, measurements, dimensions)
    - requirements (specifications, standards, compliance, regulations)
    - procedures (processes, steps, methods, instructions)
    - materials (substances, components, products, equipment)
    - timeline (deadlines, schedules, durations, phases)
    
    Query: "{query}"
    Return only the category name.
    """
    
    category = get_category_from_llm(type_prompt)
    
    # Content-based metadata filtering (language-agnostic)
    routing_config = {
        "quantities": {
            "metadata_filter": {"has_numbers": True},
            "boost_fields": ["content_length"],
            "search_weight": "precise"
        },
        "requirements": {
            "metadata_filter": {"element_category": {"$in": ["NarrativeText", "Title"]}},
            "boost_fields": ["section_title_inherited"],
            "search_weight": "comprehensive"
        },
        "procedures": {
            "metadata_filter": {"text_complexity": {"$in": ["medium", "complex"]}},
            "boost_fields": ["element_category"],
            "search_weight": "sequential"
        },
        "materials": {
            "metadata_filter": {"has_tables_on_page": True},
            "boost_fields": ["page_context"],
            "search_weight": "specific"
        },
        "timeline": {
            "metadata_filter": {"has_numbers": True, "element_category": "NarrativeText"},
            "boost_fields": ["content_length"],
            "search_weight": "contextual"
        }
    }
    
    return routing_config.get(category, routing_config["requirements"])
```

#### **Hypothetical Document Embeddings (HyDE)**
```python
def generate_hypothetical_document(query: str) -> str:
    """Generate hypothetical answer document for better embedding matching"""
    
    hyde_prompt = f"""
    Given this construction query: "{query}"
    
    Write a detailed, technical paragraph that would likely contain the answer.
    Write as if you're from an official construction document or building code.
    Include specific details, measurements, and technical language that would appear in real documents.
    
    Query: {query}
    
    Hypothetical document excerpt:
    """
    
    response = openai.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": hyde_prompt}],
        temperature=0.2  # Lower temperature for more consistent technical content
    )
    
    return response.choices[0].message.content.strip()

def create_hyde_embeddings(query: str, hypothetical_doc: str) -> Dict[str, Any]:
    """Create embeddings for both query and hypothetical document"""
    
    # Embed both the original query and hypothetical document
    voyage_client = VoyageClient(api_key=os.getenv("VOYAGE_API_KEY"))
    
    embeddings = voyage_client.embed([query, hypothetical_doc], model="voyage-multilingual-2")
    
    return {
        "query_embedding": embeddings.embeddings[0],
        "hyde_embedding": embeddings.embeddings[1],
        "hypothetical_document": hypothetical_doc
    }
```

#### **Query Processing Pipeline**
```python
class ProcessedQuery(BaseModel):
    original_query: str
    expanded_queries: List[str]                    # LLM-generated variations
    content_category: str                          # quantities/requirements/etc
    routing_config: Dict[str, Any]                 # metadata filters and weights
    hypothetical_document: Optional[str] = None    # HyDE document
    hyde_embedding: Optional[List[float]] = None   # HyDE embedding
    processing_time_ms: float
```

### **08_retrieve - Hybrid Search Implementation**

#### **Search Strategy**
- **Semantic Search**: Voyage embeddings for conceptual matching
- **Keyword Search**: BM25/TF-IDF for exact term matching  
- **Metadata Filtering**: Pre-filter by document type, complexity, source
- **Result Fusion**: Combine semantic + keyword results with weighted scoring

#### **Rich Metadata Leveraging**
- **Citation Generation**: Use `source_filename`, `page_number`, `section_title_inherited`
- **Content Type Filtering**: Filter by `element_category` (tables vs. narrative)
- **Complexity Matching**: Match query complexity to `text_complexity`
- **Quality Weighting**: Weight by `processing_strategy` confidence

#### **Retrieval Output Format**
```python
class RetrievalResult(BaseModel):
    chunk_id: str
    content: str
    score: float
    metadata: Dict[str, Any]
    citation: Dict[str, str]  # Document, page, section references
    search_type: str  # "semantic", "keyword", "hybrid"
```

### **09_rerank - Re-ranking Implementation**

#### **What Re-ranking Provides**
- **Query-Context Understanding**: Cross-encoder models process query + chunk together
- **Domain Relevance**: Construction-specific relevance beyond pure similarity
- **Quality Optimization**: Prioritize authoritative sources and clear content

#### **Implementation Strategy**
- **Model**: `ms-marco-MiniLM-L-6-v2` or `bge-reranker-base`
- **Input**: Top K results from hybrid search (e.g., top 20)
- **Output**: Re-ranked top N results (e.g., top 5-10)
- **Metadata Integration**: Boost official documents, recent content

#### **Value Demonstration Output**
```python
class RerankingAnalysis(BaseModel):
    original_ranking: List[RetrievalResult]
    reranked_results: List[RetrievalResult]
    ranking_changes: Dict[str, Any]  # Show position changes
    confidence_scores: List[float]   # Re-ranking confidence
```

### **10_context - Context Assembly & Prompt Engineering**

#### **Context Strategy**
- **Token Management**: Respect OpenAI context limits (4K/8K/32K based on model)
- **Source Attribution**: Include document references in context
- **Metadata Enhancement**: Use metadata to enrich context understanding

#### **Prompt Template Structure**
```python
SYSTEM_PROMPT = """
You are a construction industry expert assistant helping with tender bidding.
Use the provided documents to answer questions accurately and cite your sources.

When answering:
1. Provide specific, actionable information
2. Always cite document sources, pages, and sections
3. Highlight regulatory requirements vs. recommendations
4. Note any regional or temporal limitations
"""

CONTEXT_TEMPLATE = """
Relevant Documents:
{context_with_citations}

Question: {query}

Answer based on the provided documents, including specific citations:
"""
```

### **11_generate - LLM Response Generation**

#### **Model Configuration**
- **Primary Model**: OpenAI GPT-4 or GPT-3.5-turbo
- **Response Format**: Structured with citations
- **Temperature**: Low (0.1-0.3) for factual accuracy

#### **Response Enhancement**
- **Citation Integration**: Automatic source referencing
- **Confidence Indicators**: Mark uncertain information
- **Follow-up Suggestions**: Related queries for deeper exploration

### **12_evaluate - Evaluation Framework**

#### **Evaluation Dataset Creation**
1. **Historical Queries**: Real client questions from tender preparation
2. **Synthetic Generation**: GPT-4 generated Q&A from documents
3. **Expert Validation**: Construction professional review
4. **Tender-Specific Scenarios**: Actual tender requirement questions

#### **Metrics for Tender Bidding Use Case**
- **Retrieval Metrics**:
  - Precision@K, Recall@K for document retrieval
  - Citation accuracy (correct page/section references)
  - Coverage (% of ground truth sources found)

- **Generation Metrics**:
  - Answer completeness vs. ground truth
  - Citation quality and accuracy
  - Regulatory compliance coverage

- **User Experience Metrics**:
  - Time to answer vs. manual search
  - Client satisfaction scores
  - Successful bid correlation (long-term)

---

## ğŸ“š **Notebook Implementation Queue**

### **Immediate Priority (Core Functionality)**
1. **06_store**: `store_and_validate.py` - Set up Chroma storage + integrated validation
2. **07_query**: `query_processing.py` - Simple query expansion (no intent detection)
3. **08_retrieve**: `retrieve_hybrid.py` - Implement hybrid search + metadata filtering
4. **11_generate**: `generate_openai.py` - Basic OpenAI integration with citations

### **Enhancement Priority (Optimization)**
5. **09_rerank**: `rerank_analysis.py` - Re-ranking with value demonstration
6. **10_context**: `context_assembly.py` - Advanced prompt engineering
7. **12_evaluate**: `evaluate_pipeline.py` - Evaluation framework

### **Configuration Files Needed**
```
06_store/config/storage_config.json    # Includes validation settings
07_query/config/query_processing_config.json
08_retrieve/config/retrieval_config.json
09_rerank/config/reranking_config.json
10_context/config/prompt_templates.json
11_generate/config/generation_config.json
12_evaluate/config/evaluation_config.json
```

---

## **ğŸ› ï¸ Strategic Technology Stack**

### **ğŸ¯ LangChain Adoption Strategy**

```python
# Use LangChain only where it adds clear value
LANGCHAIN_COMPONENTS = [
    "08_retrieve",    # Hybrid search patterns
    "10_context",     # Prompt management  
    "12_evaluate"     # Evaluation framework
]

DIRECT_IMPLEMENTATION = [
    "06_store",       # Complex metadata needs direct control
    "07_query",       # Custom multilingual logic
    "09_rerank",      # Performance-critical
    "11_generate"     # Citation parsing needs custom logic
]

# Migration path: Later move more components to LangChain as needed
```

### **ğŸ“š Technology Stack by Component**

#### **06_store - Vector Storage + Validation (Direct Implementation)**
- **Primary**: `chromadb` - Direct client for complex metadata flattening
- **Data Models**: `pydantic` - Type safety and validation
- **Processing**: `json`, `pickle` - Loading embedded chunks
- **Testing**: `pytest` - Integrated validation testing
- **Performance**: `time` - Query response benchmarking
- **Reports**: `json` - Combined storage and validation report
- **Observability**: `langsmith` decorators for tracing
- **Why Direct**: Complex metadata flattening + performance-critical validation need precise control

#### **07_query - Language-Agnostic Processing (Direct Implementation)**
- **LLM**: `openai` - Direct API for GPT-3.5-turbo expansion & classification
- **Embeddings**: `voyageai` - For HyDE embedding generation
- **Environment**: `python-dotenv` - API key management
- **Observability**: `langsmith` decorators for tracing
- **Why Direct**: Custom multilingual logic and HyDE implementation

#### **08_retrieve - Hybrid Search (LangChain)**
- **Framework**: `langchain.retrievers` - EnsembleRetriever for hybrid search
- **Vector Search**: `langchain_chroma` - Chroma integration
- **Keyword Search**: `langchain_community.retrievers.BM25Retriever`
- **Result Fusion**: Built-in ensemble weighting
- **Why LangChain**: Proven hybrid retrieval patterns, excellent observability

#### **09_rerank - Result Optimization (Direct Implementation)**
- **Cross-Encoders**: `sentence-transformers` - Direct model access
- **Models**: `transformers` - HuggingFace model loading
- **Scoring**: `torch` - GPU-optimized model inference
- **Observability**: `langsmith` decorators for tracing
- **Why Direct**: Performance-critical, need GPU optimization control

#### **10_context - Prompt Assembly (LangChain)**
- **Framework**: `langchain.prompts` - ChatPromptTemplate management
- **Token Counting**: `tiktoken` - OpenAI token estimation
- **Template Engine**: Built-in Jinja2 support
- **Context Management**: Automatic context window handling
- **Why LangChain**: Excellent prompt management, template versioning

#### **11_generate - Response Generation (Direct Implementation)**
- **LLM**: `openai` - Direct API for GPT-4/GPT-3.5-turbo
- **Streaming**: `openai` streaming for real-time responses
- **Citation**: Custom citation parsing and formatting logic
- **Observability**: `langsmith` decorators for tracing
- **Why Direct**: Complex citation logic, custom response formatting

#### **12_evaluate - Performance Measurement (LangChain)**
- **Framework**: `langchain.evaluation` - Built-in evaluators
- **Metrics**: `ragas` - RAG evaluation framework integration
- **Datasets**: `langsmith` - Dataset management and versioning
- **Analysis**: `pandas` - Results analysis and reporting
- **Why LangChain**: Comprehensive evaluation ecosystem, dataset management

## **ğŸ”— LangSmith Integration Strategy**

### **Universal Observability**
```python
from langsmith import traceable

# All components get LangSmith tracing regardless of implementation
@traceable
def custom_storage_function():
    # Direct implementation with full observability
    pass

@traceable  
def langchain_retrieval_function():
    # LangChain components get automatic tracing
    pass
```

### **Hybrid Benefits**
- **ğŸ” Full Observability**: LangSmith tracing across all components
- **ğŸ›ï¸ Control Where Needed**: Direct implementation for complex logic
- **ğŸ§© LangChain Where Valuable**: Proven patterns for standard workflows
- **ğŸ“ˆ Migration Path**: Gradual adoption as LangChain improves

### **ğŸ“¦ Core Dependencies**
```python
# LangChain (selective usage)
langchain>=0.1.0
langchain-openai
langchain-chroma
langchain-community

# LangSmith (universal observability)
langsmith

# Direct implementation dependencies
chromadb>=0.4.0           # Vector database
openai>=1.0.0             # LLM API
voyageai>=0.2.0           # Embeddings
sentence-transformers     # Re-ranking models

# Data & ML
pydantic>=2.0.0          # Type safety
numpy                    # Numerical operations
pandas                   # Data analysis
scikit-learn            # Keyword search (if not using LangChain BM25)

# Evaluation & Testing
ragas                   # RAG evaluation
pytest                 # Testing framework

# Utilities
python-dotenv          # Environment management
tiktoken              # Token counting
```

---

## ğŸ¯ **Client-Specific Optimizations for Tender Bidding**

### **Domain Expertise Integration**
- **Regulatory Prioritization**: Weight official building codes higher
- **Geographic Filtering**: Filter by jurisdiction for local requirements
- **Project Type Matching**: Commercial vs. residential specific content
- **Temporal Relevance**: Prioritize current regulations and standards

### **Citation Excellence for Professional Use**
```python
class ProfessionalCitation(BaseModel):
    document_name: str
    section_title: str
    page_number: int
    regulatory_status: str  # "official", "guidance", "reference"
    last_updated: Optional[str]
    authority: str  # Building department, standards body, etc.
```

### **Efficiency Metrics**
- **Query Resolution Time**: Target <30 seconds for complex queries
- **Source Coverage**: Ensure comprehensive document coverage
- **Update Frequency**: Track when documents need refreshing
- **Client Success**: Measure bid win rates (long-term correlation)

---

## ğŸš€ **Next Steps**

1. **Start with 06_store**: Create Chroma storage foundation + integrated validation
2. **Add 07_query**: Implement simple query expansion 
3. **Implement 08_retrieve**: Get basic hybrid search working  
4. **Add 11_generate**: Complete end-to-end pipeline
5. **Optimize with 09_rerank**: Enhance result quality
6. **Refine with 10_context**: Polish prompt engineering
7. **Validate with 12_evaluate**: Measure and improve performance

The modular approach ensures each step can be developed and tested independently while maintaining the established patterns from your existing notebooks. 